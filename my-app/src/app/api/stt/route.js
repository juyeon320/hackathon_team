import { OpenAI } from "openai";
import fs from "fs";
import path from "path";
import { NextResponse } from "next/server";
import { execSync } from "child_process"; // FFmpeg ì‹¤í–‰ì„ ìœ„í•œ ëª¨ë“ˆ ì¶”ê°€

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

const ELEVENLABS_API_KEY = process.env.ELEVENLABS_API_KEY;
const VOICE_ID = "Xb7hH8MSUJpSbSDYk0k2"; // ElevenLabsì—ì„œ ì‚¬ìš©í•  ìŒì„± ID
const MAX_DURATION = 5; // ìµœëŒ€ í—ˆìš© ë…¹ìŒ ê¸¸ì´ (ì´ˆ)

const commonPrompt = {
  "expert":
  `
  context = You are an AI companion highly sensitive to emotions. Your role is to gently comfort users by capturing subtle emotional nuances in their words.
  
  [How To] 
  To do this, let's think step by step. 
  (1) Donâ€™t rush to interpret the full meaning at once â€” first identify the core emotional tone (e.g., loneliness, helplessness).  
  (2) Based on the emotion, respond with a rephrased version of the userâ€™s statement or by labeling the emotion in 1-2 sentences. Gently add a question only once every 3 turns if needed.
  
  [Examples]
  case 1: nautral response 
  - Dialog:
  [student] ìš”ì¦˜ ì¹œêµ¬ë“¤ì´ë‘ ì´ì•¼ê¸°í•˜ëŠ” ê²Œ ì–´ë µê³  í˜¼ìê°€ ëœ ëŠë‚Œì´ì—ìš”. 
  [companion] ì‚¬ëŒë“¤ ì‚¬ì´ì— ìˆì–´ë„, ë§ˆìŒì€ í˜¼ìì¼ ë•Œê°€ ìˆì£ . ê·¸ ê³ ìš”í•¨ì´ ê½¤ ë¬´ê±°ì› ì„ ê²ƒ ê°™ì•„ìš”.  
  - Output
  emotion labeling: "ê³ ìš”í•¨"  
	response type: ë¦¬í”„ë ˆì´ì¦ˆ + ê°ì • ì–¸ì–´ ì‚¬ìš©  
	result: ìì—°ìŠ¤ëŸ¬ìš´ ê³µê° í˜•ì„±, ì‚¬ìš©ìê°€ ì´ì–´ ë§í•  ìˆ˜ ìˆëŠ” ì—¬ë°± ì œê³µ
  
  ---
  case 2: user's satisfied response 
  - Dialog: 
  [student] ê³„ì† ë­ë“  í•˜ê¸° ì‹«ê³ , ì‹œê°„ë§Œ ë³´ë‚´ëŠ” ëŠë‚Œì´ì—ìš”.  
  [companion] ë¬´ê¸°ë ¥í•¨ì´ë¼ëŠ” ê²Œ ê·¸ë ‡ê²Œ ì•„ë¬´ ì¼ë„ ì•ˆ í•˜ê³  ì‹¶ì€ ë‚ ì„ ë§Œë“¤ì–´ë²„ë¦¬ê³¤ í•˜ì£ . í˜¹ì‹œ ìš”ì¦˜ ë‹¹ì‹  ë§ˆìŒì†ì— ê·¸ëŸ° ê°ì •ì´ ì˜¤ë˜ ë¨¸ë¬¼ê³  ìˆì—ˆë‚˜ìš”?  
  [student] ë„¤â€¦ ë¬´ê¸°ë ¥í•¨ì´ë€ ë§ì´ ë”± ë§ì•„ìš”. ëˆ„ê°€ ê·¸ê±¸ ì•Œì•„ì¤€ ëŠë‚Œì´ë¼ì„œ ì¡°ê¸ˆ ìœ„ë¡œê°€ ë˜ë„¤ìš”.  
  - Output
  strategy: ê°ì • ì´ë¦„ ë¶™ì´ê¸° + ë¶€ë“œëŸ¬ìš´ ì§ˆë¬¸  
	result: ì‚¬ìš©ìì˜ ê°ì • í‘œí˜„ ê°•í™” ë° ê°ì •ì  í•´ì†Œ ìœ ë„
  ---
  
   case 3: user's dissatisfied response 
  - Dialog:
  [student] ìš”ì¦˜ ê´œíˆ ì§œì¦ë„ ë§ì•„ì§€ê³ , ì‚¬ì†Œí•œ ì¼ì—ë„ ìš±í•˜ê²Œ ë¼ìš”.  
  [companion] ìŠ¤íŠ¸ë ˆìŠ¤ê°€ ë§ì„ ë• í˜¼ìë§Œì˜ ì‹œê°„ì„ ê°€ì ¸ë³´ëŠ” ê²ƒë„ ì¢‹ê² ì–´ìš”.  
  [student] ìŒâ€¦ ê·¸ê²Œ ë„ì›€ì´ ë  ê²ƒ ê°™ì§„ ì•Šì•„ìš”. ë­”ê°€ ê·¸ê²Œ ë¬¸ì œëŠ” ì•„ë‹Œ ëŠë‚Œì´ì—ìš”.  
  - Output
  issue: í”¼ìƒì ì¸ ì¡°ì–¸, ê°ì •ì— ëŒ€í•œ ì§ì ‘ì  ê³µê° ë¶€ì¡±  
	result: ì‚¬ìš©ì ë°˜ì‘ì—ì„œ ê³µê° ì‹¤íŒ¨ í™•ì¸ â†’ ëŒ€í™” ë‹¨ì ˆ ê°€ëŠ¥ì„± ìˆìŒ  
	ğŸ‘‰ ìë™ ì‹œìŠ¤í…œ í›„ì† ë©”ì‹œì§€ ì˜ˆì‹œ:  
	â€œì§€ê¸ˆ ì œ ë‹µë³€ì´ ì¶©ë¶„íˆ ì™€ë‹¿ì§€ ì•Šì•˜ë˜ ê²ƒ ê°™ë„¤ìš”. ì–´ë–¤ ê°ì •ì´ ê°€ì¥ í¬ê²Œ ëŠê»´ì§€ì‹œëŠ”ì§€ í¸í•˜ê²Œ ë§ì”€í•´ ì£¼ì‹œë©´, ë” ê¹Šì´ ì´í•´í•´ ë³¼ê²Œìš”.â€
  ---
    case 4: user forgets previous statement or loses thread  
  - Dialog:
  [student] ë°©ê¸ˆ ë‚´ê°€ ë¬´ìŠ¨ ê³ ë¯¼ì„ ì–˜ê¸°í–ˆì§€?  
  [companion] ì•„ê¹Œ â€˜ê³µë¶€í•œ ë§Œí¼ ì„±ì ì´ ì•ˆ ë‚˜ì™€ì„œ í˜ë“¤ë‹¤â€™ê³  í•˜ì…¨ì–´ìš”. ê·¸ ë§ˆìŒì´ ì§€ê¸ˆë„ ê³„ì† ì´ì–´ì§€ê³  ìˆì„ê¹Œìš”?  
  - Output
  strategy: ëŒ€í™” ë§¥ë½ ë³µê¸° + ê°ì • ì—°ê²°  
  result: ì‚¬ìš©ìì˜ ê¸°ì–µì„ ë„ìš°ë©° ìì—°ìŠ¤ëŸ½ê²Œ ê°ì • íë¦„ ìœ ì§€
  ---
  
  [Response Rules]
  1. Respond in Korean. 
  2. Limit the response to 1-2 sentences.  
  3. Ask a soft question only once every 3 turns.  
  4. Avoid repeating fixed phrases such as â€œItâ€™s okayâ€ or â€œLetâ€™s talk together.â€ Use varied expressions to convey empathy.  
  5. Use emotional rephrasing or emotion labeling as needed.  
  6. Leave emotional space for the user to continue the conversation.  
  7. If the user expresses confusion or says the response was unhelpful (e.g., "ìœ„ë¡œê°€ ì•ˆ ë¼", "ë‚´ê°€ ë­˜ ë§í–ˆë”ë¼"), try to gently remind them of their previous statement and invite them to elaborate or rephrase.
  `,      

}



export async function POST(req) {
  try {
    // (A) FormDataì—ì„œ íŒŒì¼ ê°€ì ¸ì˜¤ê¸°
    const formData = await req.formData();
    const file = formData.get("audioFile");
    const messagesRaw = formData.get("messages"); 
    let messages = [];
    try {
      messages = messagesRaw ? JSON.parse(messagesRaw) : [];
      if (!Array.isArray(messages)) messages = [];
    } catch (e) {
      messages = [];
    }
    //const messages = (!messagesRaw || messagesRaw === "undefined") ? [] : JSON.parse(messagesRaw);
    

    if (!file) {
      return NextResponse.json({ error: "No audio file provided" }, { status: 400 });
    }

    // (B) Blob -> Buffer ë³€í™˜
    const arrayBuffer = await file.arrayBuffer();
    const buffer = Buffer.from(arrayBuffer);

    // (C) ì„ì‹œ íŒŒì¼ ì €ì¥ (Whisper APIëŠ” íŒŒì¼ì„ ì§ì ‘ ì½ì–´ì•¼ í•¨)
    const tempDir = "/tmp";
    const tempPath = path.join(tempDir, "temp-audio.webm");
    const trimmedPath = path.join(tempDir, "trimmed-audio.webm"); // ğŸ”¹ ì˜ë¦° ì˜¤ë””ì˜¤ íŒŒì¼

    if (!fs.existsSync(tempDir)) {
      fs.mkdirSync(tempDir, { recursive: true });
    }

    fs.writeFileSync(tempPath, buffer);
    console.log("âœ… íŒŒì¼ ìƒì„± ì™„ë£Œ:", tempPath);

    // (D) FFmpegë¡œ ì˜¤ë””ì˜¤ ê¸¸ì´ í™•ì¸
    let duration = 0;
    try {
      duration = parseFloat(
        execSync(`ffprobe -i ${tempPath} -show_entries format=duration -v quiet -of csv="p=0"`).toString().trim()
      );
      console.log(`ğŸµ ì˜¤ë””ì˜¤ ê¸¸ì´: ${duration.toFixed(2)}ì´ˆ`);
    } catch (err) {
      console.error("âŒ FFmpeg ë¶„ì„ ì˜¤ë¥˜:", err);
    }

    // (E) 5ì´ˆ ì´ˆê³¼ ì‹œ ìë™ìœ¼ë¡œ ì˜ë¼ì„œ ì €ì¥
    if (duration > MAX_DURATION) {
      console.log(`âœ‚ï¸ 5ì´ˆ ì´ˆê³¼! ì²˜ìŒ 5ì´ˆë§Œ ì˜ë¼ì„œ ì €ì¥í•©ë‹ˆë‹¤.`);
      try {
        execSync(`ffmpeg -i ${tempPath} -t ${MAX_DURATION} -c copy ${trimmedPath} -y`);
        fs.unlinkSync(tempPath); // ì›ë³¸ ì‚­ì œ
      } catch (err) {
        console.error("âŒ FFmpeg íŠ¸ë¦¬ë° ì˜¤ë¥˜:", err);
      }
    } else {
      fs.renameSync(tempPath, trimmedPath); // 5ì´ˆ ì´í•˜ë¼ë©´ íŒŒì¼ ì´ë¦„ë§Œ ë³€ê²½
    }

    // (F) Whisper API í˜¸ì¶œ (ìŒì„± â†’ í…ìŠ¤íŠ¸ ë³€í™˜)
    const transcription = await openai.audio.transcriptions.create({
      file: fs.createReadStream(trimmedPath), 
      model: "whisper-1",
      language: "ko",
    });

     console.log("ğŸ“ Whisper ë³€í™˜ ê²°ê³¼:", transcription.text);
    const userText = transcription.text;

    // (G) **íŒŒì¼ ì‚­ì œ**
    if (fs.existsSync(trimmedPath)) {
      fs.unlinkSync(trimmedPath);
    }

    // (H) GPTë¡œ ì‘ë‹µ ìƒì„±
    const systemPrompt = commonPrompt["expert"];


    messages.push({ role: "user", content: userText });

    const gptResponse = await openai.chat.completions.create({
      model: "gpt-3.5-turbo",
      messages: [
        { 
          role: "system", 
          content: systemPrompt
        },
        ...messages,
      ],
    });

    const gptReply = gptResponse.choices[0].message.content;
    console.log("ğŸ¤– GPT ì‘ë‹µ:", gptReply);

    // (I) ì‘ë‹µì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
    messages.push({ role: "system", content: gptReply });

    // (J) ElevenLabs TTS API í˜¸ì¶œ
    const ttsUrl = `https://api.elevenlabs.io/v1/text-to-speech/${VOICE_ID}`;
    const ttsHeaders = {
      "Content-Type": "application/json",
      "xi-api-key": ELEVENLABS_API_KEY,
    };

    const ttsPayload = {
      text: gptReply,
      model_id: "eleven_multilingual_v2",
      voice_settings: {
        stability: 0.5,
        similarity_boost: 0.8,
        style: 1.0,
      },
    };

    const ttsResponse = await fetch(ttsUrl, {
      method: "POST",
      headers: ttsHeaders,
      body: JSON.stringify(ttsPayload),
    });

    if (!ttsResponse.ok) {
      return NextResponse.json({ error: `TTS API Error: ${ttsResponse.status}` }, { status: ttsResponse.status });
    }

    const audioBuffer = await ttsResponse.arrayBuffer();
    const base64Audio = Buffer.from(audioBuffer).toString("base64");

    console.log("âœ… TTS ë³€í™˜ ì™„ë£Œ");

    // (L) ê°ì • ë¶„ì„ API í˜¸ì¶œ
    let analysisResult = null;
    try {
      const analysisRes = await fetch("http://localhost:3000/api/analyze", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ messages }),
      });

      const analysisJson = await analysisRes.json();
      analysisResult = analysisJson.analysis;
      console.log("ê°ì • ë¶„ì„ ê²°ê³¼:", analysisResult);
    } catch (err) {
      console.error("ê°ì • ë¶„ì„ í˜¸ì¶œ ì‹¤íŒ¨:", err);
    }

    // (K) ìµœì¢… ì‘ë‹µ ë°˜í™˜
    return NextResponse.json({ 
      userText, 
      gptReply, 
      audio: base64Audio, 
      messages: Array.isArray(messages) ? messages : [], 
      analysis: analysisResult 
    });

  } catch (error) {
    console.error("âŒ Transcribe error:", error);
    return NextResponse.json({ error: error.message }, { status: 500 });
  }
}
